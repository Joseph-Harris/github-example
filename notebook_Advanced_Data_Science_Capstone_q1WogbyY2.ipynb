{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Advanced Data Science Capstone"}, {"metadata": {}, "cell_type": "markdown", "source": "## Joseph Harris"}, {"metadata": {}, "cell_type": "markdown", "source": "##  Artist Recommender System\nIn this capstone we will be using a system that recommends, or predicts, what a user may wants or not.  The purpose of this helps users find something they may not have found on their own.  Examples include Netflix (which suggests movies based on other movies you watch), Twitter (which reccomends other accounts based on who you follow), and Amazon (which reccomends items based on what you bought before).\n\nWe will show, in this Capstone, how to build a recommender syster. We will focus on music and use alternating least squares (ALS), a simple algorithm to make predictions."}, {"metadata": {}, "cell_type": "markdown", "source": "## Goals\nIn no particular order:\n1. Revisit (or learn) recommender algorithms\n2. Understand the idea of Matrix Factorization and the ALS algorithm (serial and parallel versions)\n3. Build a simple model for a real usecase: music recommender system\n4. Understand how to validate the results"}, {"metadata": {}, "cell_type": "markdown", "source": "## Steps\n1. Using Spark SQL, we will inspect the data and build basic, but valuable, knowledge about the information we have at hand\n2. Define what algorithm will work. Something that would build a statistical model to predict tastes, of music in this case.\n3. Using different methods to use the algorithm.\n4. Focus on an existing implementation that we will use out-of-the-box to build a statistical model.\n\n### Furthermore:\nWe will cover how to validate our results and how to choose the best parameters to train models.  We will focus on the statistical validation of our system."}, {"metadata": {"trusted": false}, "cell_type": "markdown", "source": "## 1. Data\n"}, {"metadata": {}, "cell_type": "code", "source": "# Downloading dataset\n!set -e\n\n!mkdir -p dataset\n!cd dataset\n!wget http://www.iro.umontreal.ca/~lisa/datasets/profiledata_06-May-2005.tar.gz\n!tar xvf profiledata_06-May-2005.tar.gz\n!mv profiledata_06-May-2005/* dataset/\n!rm -r profiledata_06-May-2005", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "--2020-05-21 22:48:31--  http://www.iro.umontreal.ca/~lisa/datasets/profiledata_06-May-2005.tar.gz\nResolving www.iro.umontreal.ca (www.iro.umontreal.ca)... 132.204.26.36\nConnecting to www.iro.umontreal.ca (www.iro.umontreal.ca)|132.204.26.36|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 135880312 (130M) [application/x-gzip]\nSaving to: \u2018profiledata_06-May-2005.tar.gz\u2019\n\n100%[======================================>] 135,880,312 29.8MB/s   in 5.1s   \n\n2020-05-21 22:48:37 (25.6 MB/s) - \u2018profiledata_06-May-2005.tar.gz\u2019 saved [135880312/135880312]\n\nprofiledata_06-May-2005/\nprofiledata_06-May-2005/artist_data.txt\nprofiledata_06-May-2005/README.txt\nprofiledata_06-May-2005/user_artist_data.txt\nprofiledata_06-May-2005/artist_alias.txt\n", "name": "stdout"}]}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "!cat dataset/user_artist_data.txt | head", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "1000002 1 55\r\n1000002 1000006 33\r\n1000002 1000007 8\r\n1000002 1000009 144\r\n1000002 1000010 314\r\n1000002 1000013 8\r\n1000002 1000014 42\r\n1000002 1000017 69\r\n1000002 1000024 329\r\n1000002 1000025 1\r\ncat: write error: Broken pipe\r\n", "name": "stdout"}]}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "!pip install findspark", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: findspark in /opt/conda/envs/Python36/lib/python3.6/site-packages (1.3.0)\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "os.environ['SPARK_HOME'] = \"/usr/local/Cellar/apache-spark/2.2.0/libexec/\"\n\n\nsys.path.append(\"D:\\python\\spark-1.4.1-bin-hadoop2.4\\python\")\nsys.path.append(\"D:\\python\\spark-1.4.1-bin-hadoop2.4\\python\\lib\\py4j-0.8.2.1-src.zip\")", "execution_count": 36, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import findspark\nfindspark.init('/usr/local/Cellar/apache-spark/2.2.0/libexec')", "execution_count": 37, "outputs": [{"output_type": "error", "ename": "IndexError", "evalue": "list index out of range", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)", "\u001b[0;32m<ipython-input-37-f64700909d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/Cellar/apache-spark/2.2.0/libexec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/opt/conda/envs/Python36/lib/python3.6/site-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;31m# add pyspark to sys.path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'py4j-*.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}, {"metadata": {}, "cell_type": "code", "source": "import os\nimport sys\nimport re\nimport random", "execution_count": 38, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import *\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom time import time\n\nsqlContext = SQLContext(sc)\n# Base directory path\nbase = \"dataset/\"", "execution_count": 39, "outputs": [{"output_type": "error", "ename": "ModuleNotFoundError", "evalue": "No module named 'pyspark'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "\u001b[0;32m<ipython-input-39-34e69a2c0aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"]}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}